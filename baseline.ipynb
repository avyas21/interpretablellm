{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f343ad35-e195-4859-a3a7-35e4120d1b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: packaging>=20.0 in /home/akhilvimalvyas/.local/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 KB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/akhilvimalvyas/.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting huggingface-hub<1.0,>=0.24.0\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.1/464.1 KB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.0/462.0 KB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3/dist-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/akhilvimalvyas/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2025.2.0-py3-none-any.whl (184 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.5/184.5 KB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/akhilvimalvyas/.local/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Installing collected packages: tqdm, safetensors, regex, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.17.0 fsspec-2025.2.0 huggingface-hub-0.28.1 regex-2024.11.6 safetensors-0.5.2 tokenizers-0.21.0 tqdm-4.67.1 transformers-4.48.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install numpy==1.26.0\n",
    "!pip install tensorflow[and-cuda]\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install --upgrade ipywidgets\n",
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2962b9d0-ba93-4ffa-bf14-18aa63fedea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 17 04:11:16 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   35C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b82dd9f-e6d6-46ac-9843-ca25199683dc",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c594c372-0387-4956-b9e6-a362de13ab1b",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcc1073e-f664-4053-a2a0-6a866a8ba2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 04:26:00.925187: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739766360.948186    3077 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739766360.955177    3077 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-17 04:26:00.980890: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3efd9030-e3ac-4c20-92b5-a82df8b63a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b73df76d7f4a699909eb7e31dd3b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f27b92f2bb43a5994d9c03d3de76f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1bb822cf54640a4a34f409f0b03e570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11baef684acb41fa9bb96df4d52fc72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15673661debd42aab32e276182b94950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1993, -0.2101, -0.1950,  ..., -0.4733,  0.0861,  0.7103],\n",
      "         [-0.5400, -0.7178, -0.2873,  ..., -0.7211,  0.5801,  0.3946],\n",
      "         [-0.1421, -0.7375,  0.3737,  ..., -0.3740,  0.0750,  0.9687],\n",
      "         ...,\n",
      "         [ 0.1321, -0.2893, -0.0043,  ..., -0.1772, -0.2123, -0.1983],\n",
      "         [ 0.4060,  0.0366, -0.7327,  ...,  0.4169, -0.3416, -0.4542],\n",
      "         [ 0.0646, -0.2088, -0.1323,  ...,  0.5954, -1.0679,  0.0173]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-9.4299e-01, -5.7258e-01, -8.9093e-01,  8.4917e-01,  7.3438e-01,\n",
      "         -3.1424e-01,  9.3282e-01,  4.1172e-01, -8.2343e-01, -1.0000e+00,\n",
      "         -3.7284e-01,  9.3451e-01,  9.8351e-01,  5.0457e-01,  9.4685e-01,\n",
      "         -7.9498e-01, -3.7354e-01, -6.6457e-01,  4.4397e-01, -7.5095e-01,\n",
      "          7.2463e-01,  9.9999e-01,  3.0325e-01,  4.2836e-01,  5.6970e-01,\n",
      "          9.8481e-01, -7.9837e-01,  9.3836e-01,  9.6563e-01,  7.7279e-01,\n",
      "         -8.2288e-01,  2.7461e-01, -9.9091e-01, -3.3512e-01, -9.2995e-01,\n",
      "         -9.9428e-01,  5.2797e-01, -7.9985e-01, -1.5079e-01, -9.3853e-02,\n",
      "         -9.2339e-01,  4.5827e-01,  9.9999e-01, -3.2961e-01,  4.7323e-01,\n",
      "         -4.3278e-01, -1.0000e+00,  4.0610e-01, -9.1698e-01,  9.0670e-01,\n",
      "          8.8102e-01,  8.5099e-01,  3.3882e-01,  6.2175e-01,  6.2976e-01,\n",
      "         -2.9181e-01,  4.4824e-02,  2.9721e-01, -3.6886e-01, -7.0939e-01,\n",
      "         -7.0921e-01,  4.3761e-01, -7.8405e-01, -9.4754e-01,  8.6314e-01,\n",
      "          8.1323e-01, -2.7983e-01, -4.5793e-01, -3.0205e-01,  3.4432e-02,\n",
      "          9.3495e-01,  4.0481e-01, -2.2344e-01, -8.3385e-01,  7.0226e-01,\n",
      "          4.0168e-01, -6.3696e-01,  1.0000e+00, -6.1509e-01, -9.8158e-01,\n",
      "          8.3299e-01,  7.7479e-01,  6.1630e-01, -3.5069e-01,  5.9604e-01,\n",
      "         -1.0000e+00,  6.9729e-01, -2.4506e-01, -9.8932e-01,  2.3893e-01,\n",
      "          6.4326e-01, -3.9268e-01,  7.6987e-01,  6.5098e-01, -6.7676e-01,\n",
      "         -5.1638e-01, -4.9274e-01, -8.3623e-01, -3.5879e-01, -4.4271e-01,\n",
      "          2.2667e-01, -4.4745e-01, -5.1476e-01, -4.5929e-01,  4.5641e-01,\n",
      "         -5.8722e-01, -5.5043e-01,  6.7926e-01,  1.6857e-01,  8.1656e-01,\n",
      "          4.8666e-01, -4.8503e-01,  5.6523e-01, -9.6008e-01,  6.9900e-01,\n",
      "         -5.0323e-01, -9.8866e-01, -6.2076e-01, -9.8676e-01,  7.9322e-01,\n",
      "         -2.0863e-01, -4.1675e-01,  9.7124e-01, -2.9989e-01,  4.9019e-01,\n",
      "         -2.9001e-01, -9.1348e-01, -1.0000e+00, -5.8614e-01, -6.6061e-01,\n",
      "         -2.6694e-01, -3.9146e-01, -9.8190e-01, -9.6048e-01,  7.5962e-01,\n",
      "          9.7284e-01,  3.3958e-01,  9.9994e-01, -4.4164e-01,  9.5136e-01,\n",
      "         -3.0791e-01, -6.8793e-01,  4.4535e-01, -5.8097e-01,  8.1467e-01,\n",
      "          5.7327e-01, -7.9696e-01,  2.9037e-01, -2.9141e-01,  4.3167e-01,\n",
      "         -7.1466e-01, -4.1172e-01, -8.3761e-01, -9.2677e-01, -4.8237e-01,\n",
      "          9.4554e-01, -5.3289e-01, -9.3398e-01,  8.6824e-02, -3.6185e-01,\n",
      "         -4.9015e-01,  8.9939e-01,  7.6645e-01,  4.9538e-01, -3.7162e-01,\n",
      "          5.9488e-01,  5.5546e-01,  6.5237e-01, -9.2901e-01,  4.2131e-02,\n",
      "          5.8164e-01, -3.5248e-01, -8.7799e-01, -9.8223e-01, -4.9238e-01,\n",
      "          6.1011e-01,  9.8879e-01,  8.1970e-01,  4.4426e-01,  8.2262e-01,\n",
      "         -4.9194e-01,  7.2638e-01, -9.5643e-01,  9.8360e-01, -2.9188e-01,\n",
      "          3.6624e-01, -5.2040e-01,  5.7776e-01, -9.1527e-01,  2.4354e-01,\n",
      "          9.0106e-01, -6.0003e-01, -8.9684e-01, -2.3443e-01, -6.2698e-01,\n",
      "         -5.5630e-01, -8.0445e-01,  6.1571e-01, -4.9513e-01, -4.7420e-01,\n",
      "         -2.6719e-01,  9.3109e-01,  9.8936e-01,  7.7224e-01,  2.3902e-01,\n",
      "          6.7729e-01, -9.1213e-01, -5.6562e-01,  2.0625e-01,  3.9076e-01,\n",
      "          2.5318e-01,  9.9405e-01, -7.1795e-01, -2.9662e-01, -9.5165e-01,\n",
      "         -9.8491e-01,  3.8105e-02, -9.0007e-01, -2.4137e-01, -7.7969e-01,\n",
      "          7.3353e-01, -4.1894e-01,  5.5199e-01,  4.8871e-01, -9.9362e-01,\n",
      "         -8.8404e-01,  4.4750e-01, -5.5172e-01,  6.1803e-01, -3.7621e-01,\n",
      "          8.7174e-01,  9.5399e-01, -7.4851e-01,  7.8246e-01,  9.4368e-01,\n",
      "         -8.7716e-01, -8.1415e-01,  8.9846e-01, -4.2138e-01,  9.3926e-01,\n",
      "         -7.6084e-01,  9.9612e-01,  9.0972e-01,  7.6524e-01, -9.3710e-01,\n",
      "         -7.6951e-01, -9.4897e-01, -7.7909e-01, -2.8920e-01,  1.7347e-01,\n",
      "          8.8910e-01,  7.0076e-01,  4.6642e-01,  1.5714e-01, -7.0514e-01,\n",
      "          9.9919e-01, -8.7127e-01, -9.5810e-01, -2.8224e-01, -2.4028e-01,\n",
      "         -9.9094e-01,  8.8652e-01,  3.2522e-01,  3.7287e-01, -6.2558e-01,\n",
      "         -7.2060e-01, -9.6744e-01,  9.1776e-01,  2.1106e-01,  9.9480e-01,\n",
      "         -5.2141e-01, -9.4396e-01, -7.4704e-01, -9.2990e-01, -3.6372e-02,\n",
      "         -4.2624e-01, -3.1322e-01, -7.3688e-05, -9.5495e-01,  6.3036e-01,\n",
      "          6.3409e-01,  5.8571e-01, -8.5348e-01,  9.9941e-01,  1.0000e+00,\n",
      "          9.7777e-01,  8.8965e-01,  9.3697e-01, -9.9989e-01, -6.0251e-01,\n",
      "          1.0000e+00, -9.9356e-01, -1.0000e+00, -9.5697e-01, -7.7455e-01,\n",
      "          5.0010e-01, -1.0000e+00, -3.4364e-01, -9.9068e-02, -9.1489e-01,\n",
      "          6.7228e-01,  9.8295e-01,  9.9672e-01, -1.0000e+00,  8.9718e-01,\n",
      "          9.4504e-01, -6.8782e-01,  9.3593e-01, -5.7071e-01,  9.7793e-01,\n",
      "          7.2656e-01,  5.8319e-01, -3.5341e-01,  4.9573e-01, -9.3586e-01,\n",
      "         -9.0961e-01, -6.7200e-01, -7.4322e-01,  9.9877e-01,  2.4389e-01,\n",
      "         -8.3662e-01, -9.2477e-01,  5.6935e-01, -1.2380e-01,  3.1797e-02,\n",
      "         -9.6293e-01, -3.4256e-01,  4.8209e-01,  8.4231e-01,  4.0033e-01,\n",
      "          4.0712e-01, -8.0011e-01,  4.7251e-01,  2.3256e-01,  5.1941e-01,\n",
      "          6.8988e-01, -9.4960e-01, -6.5766e-01, -1.7878e-01, -2.0912e-01,\n",
      "         -6.3883e-01, -9.7535e-01,  9.7274e-01, -4.9717e-01,  8.7959e-01,\n",
      "          1.0000e+00,  3.7092e-01, -8.9917e-01,  6.6480e-01,  3.5943e-01,\n",
      "         -6.4960e-01,  1.0000e+00,  7.9131e-01, -9.8566e-01, -6.1673e-01,\n",
      "          7.1090e-01, -6.9198e-01, -7.0822e-01,  9.9973e-01, -4.4506e-01,\n",
      "         -7.3371e-01, -4.7341e-01,  9.7718e-01, -9.9224e-01,  9.9549e-01,\n",
      "         -9.1442e-01, -9.6908e-01,  9.6958e-01,  9.4492e-01, -6.3145e-01,\n",
      "         -8.0897e-01,  2.5237e-01, -8.2719e-01,  4.2726e-01, -9.7301e-01,\n",
      "          7.9668e-01,  5.4817e-01, -2.1784e-01,  8.9414e-01, -8.9280e-01,\n",
      "         -6.4768e-01,  5.4165e-01, -6.1155e-01, -6.6736e-03,  9.5998e-01,\n",
      "          5.9635e-01, -3.9007e-01,  1.6957e-01, -4.0874e-01, -5.7668e-01,\n",
      "         -9.7191e-01,  5.3368e-01,  1.0000e+00, -2.8201e-01,  6.8223e-01,\n",
      "         -4.9309e-01, -2.0582e-01,  5.5119e-02,  6.6335e-01,  7.1513e-01,\n",
      "         -4.2704e-01, -9.0475e-01,  8.5073e-01, -9.7213e-01, -9.8840e-01,\n",
      "          7.9919e-01,  3.4014e-01, -4.5839e-01,  1.0000e+00,  6.0477e-01,\n",
      "          2.5195e-01,  4.7606e-01,  9.8021e-01,  1.7525e-01,  6.0231e-01,\n",
      "          8.4835e-01,  9.8093e-01, -4.2150e-01,  6.1582e-01,  9.0732e-01,\n",
      "         -9.0096e-01, -4.5042e-01, -7.9273e-01,  6.5279e-02, -9.5087e-01,\n",
      "         -1.1438e-01, -9.6163e-01,  9.7313e-01,  9.0516e-01,  4.9724e-01,\n",
      "          3.9209e-01,  7.8612e-01,  1.0000e+00, -3.4641e-01,  7.3736e-01,\n",
      "         -4.8711e-01,  8.8986e-01, -9.9986e-01, -8.9589e-01, -5.2291e-01,\n",
      "         -2.1567e-01, -7.5208e-01, -4.3104e-01,  4.1335e-01, -9.6966e-01,\n",
      "          7.9161e-01,  6.4767e-01, -9.9344e-01, -9.8960e-01, -2.4173e-01,\n",
      "          8.8211e-01,  1.5753e-01, -9.8476e-01, -7.1690e-01, -6.7584e-01,\n",
      "          6.9141e-01, -4.7022e-01, -9.2647e-01, -9.9641e-02, -4.9025e-01,\n",
      "          5.6206e-01, -4.6670e-01,  6.3577e-01,  8.8391e-01,  5.7393e-01,\n",
      "         -8.5811e-01, -2.5894e-01, -1.9639e-01, -8.4241e-01,  8.6346e-01,\n",
      "         -8.8620e-01, -9.1561e-01, -2.6486e-01,  1.0000e+00, -7.0048e-01,\n",
      "          9.2459e-01,  8.0708e-01,  8.0524e-01, -3.4788e-01,  2.5535e-01,\n",
      "          9.4768e-01,  3.5077e-01, -8.0590e-01, -8.5887e-01, -7.5780e-01,\n",
      "         -4.8603e-01,  7.4089e-01,  5.4085e-01,  8.1245e-01,  8.4715e-01,\n",
      "          8.3696e-01,  1.8781e-01, -1.4961e-01,  2.1405e-01,  9.9985e-01,\n",
      "         -2.5312e-01, -4.2248e-01, -7.1812e-01, -2.2723e-01, -4.5109e-01,\n",
      "         -2.9949e-01,  1.0000e+00,  4.8099e-01,  3.6256e-01, -9.9149e-01,\n",
      "         -8.7700e-01, -9.4159e-01,  1.0000e+00,  8.3847e-01, -8.7695e-01,\n",
      "          8.0982e-01,  5.4304e-01, -2.5525e-01,  8.1958e-01, -3.9162e-01,\n",
      "         -3.6645e-01,  2.9118e-01,  2.3498e-01,  9.5400e-01, -6.7039e-01,\n",
      "         -9.6634e-01, -7.2011e-01,  5.4097e-01, -9.6847e-01,  9.9994e-01,\n",
      "         -7.7072e-01, -3.9436e-01, -5.1865e-01, -2.1231e-01,  6.9002e-01,\n",
      "          9.5955e-02, -9.8076e-01, -3.0263e-01,  2.3811e-01,  9.7268e-01,\n",
      "          4.0723e-01, -6.2987e-01, -9.3956e-01,  8.2620e-01,  7.7330e-01,\n",
      "         -8.5822e-01, -9.3199e-01,  9.6850e-01, -9.8046e-01,  6.2411e-01,\n",
      "          1.0000e+00,  5.4706e-01, -6.8840e-02,  3.8605e-01, -6.5183e-01,\n",
      "          4.6342e-01, -2.8667e-01,  7.6140e-01, -9.6358e-01, -5.2746e-01,\n",
      "         -4.1265e-01,  4.5434e-01, -3.3552e-01, -2.6719e-01,  6.9705e-01,\n",
      "          2.9986e-01, -5.6544e-01, -6.8939e-01, -3.4395e-01,  5.4504e-01,\n",
      "          8.8369e-01, -3.2734e-01, -3.1823e-01,  1.9555e-01, -3.3232e-01,\n",
      "         -9.2918e-01, -4.7741e-01, -6.0873e-01, -1.0000e+00,  7.5087e-01,\n",
      "         -1.0000e+00,  5.7610e-01,  2.0790e-01, -3.1736e-01,  8.7145e-01,\n",
      "          6.0871e-01,  7.7120e-01, -8.0965e-01, -8.1214e-01,  3.5827e-01,\n",
      "          7.9765e-01, -4.7965e-01, -2.8906e-01, -7.5634e-01,  4.7069e-01,\n",
      "         -2.3533e-01,  3.8380e-01, -6.1306e-01,  8.2071e-01, -3.8124e-01,\n",
      "          1.0000e+00,  2.4438e-01, -7.6696e-01, -9.8557e-01,  3.9646e-01,\n",
      "         -4.1885e-01,  1.0000e+00, -9.6038e-01, -9.6126e-01,  4.5108e-01,\n",
      "         -8.3281e-01, -8.5607e-01,  4.6982e-01,  2.2935e-01, -8.1769e-01,\n",
      "         -9.6042e-01,  9.6643e-01,  9.3020e-01, -6.0056e-01,  5.8753e-01,\n",
      "         -4.2803e-01, -6.4833e-01,  1.1353e-01,  8.4492e-01,  9.8801e-01,\n",
      "          6.4064e-01,  9.5517e-01,  1.1137e-01, -4.4765e-01,  9.7652e-01,\n",
      "          4.2299e-01,  6.4696e-01,  2.1821e-01,  1.0000e+00,  4.4588e-01,\n",
      "         -9.5404e-01,  1.0842e-01, -9.9031e-01, -3.7959e-01, -9.7207e-01,\n",
      "          4.1987e-01,  4.1573e-01,  9.1830e-01, -4.3177e-01,  9.7062e-01,\n",
      "         -8.1857e-01,  1.9719e-01, -5.3266e-01, -4.2440e-01,  4.6422e-01,\n",
      "         -9.3207e-01, -9.8856e-01, -9.8420e-01,  6.8208e-01, -5.7221e-01,\n",
      "         -2.4689e-01,  3.8592e-01,  2.5520e-01,  5.8242e-01,  5.0715e-01,\n",
      "         -1.0000e+00,  9.4845e-01,  5.9428e-01,  9.1268e-01,  9.5631e-01,\n",
      "          6.2173e-01,  6.0834e-01,  3.5926e-01, -9.8728e-01, -9.8819e-01,\n",
      "         -5.2150e-01, -4.8239e-01,  8.3559e-01,  7.1770e-01,  9.1170e-01,\n",
      "          5.0261e-01, -5.6209e-01, -6.2910e-01, -6.3388e-01, -7.2700e-01,\n",
      "         -9.9325e-01,  5.2566e-01, -6.3572e-01, -9.7949e-01,  9.6491e-01,\n",
      "          1.0157e-01, -2.8965e-01, -2.5127e-02, -8.1038e-01,  9.5513e-01,\n",
      "          8.6251e-01,  5.0891e-01,  1.3635e-01,  6.1807e-01,  9.0050e-01,\n",
      "          9.7696e-01,  9.8731e-01, -8.3567e-01,  8.6648e-01, -5.7461e-01,\n",
      "          6.4811e-01,  6.4998e-01, -9.4729e-01,  3.6252e-01,  5.9195e-01,\n",
      "         -4.9330e-01,  3.7550e-01, -3.8633e-01, -9.8395e-01,  6.3594e-01,\n",
      "         -3.8721e-01,  6.1967e-01, -5.4359e-01, -5.8517e-02, -5.3679e-01,\n",
      "         -3.5283e-01, -8.6642e-01, -7.6552e-01,  6.6115e-01,  5.5424e-01,\n",
      "          9.0701e-01,  8.6111e-01, -2.5044e-01, -7.9111e-01, -3.5136e-01,\n",
      "         -8.3013e-01, -9.4113e-01,  9.6203e-01, -1.2759e-01, -4.7110e-01,\n",
      "          7.5167e-01,  1.0877e-01,  8.3754e-01,  2.4697e-01, -5.4928e-01,\n",
      "         -4.2823e-01, -8.6458e-01,  9.1330e-01, -5.3473e-01, -6.7663e-01,\n",
      "         -6.3501e-01,  7.1364e-01,  4.4918e-01,  9.9999e-01, -8.1240e-01,\n",
      "         -8.7004e-01, -4.4191e-01, -4.5436e-01,  5.2397e-01, -5.3613e-01,\n",
      "         -1.0000e+00,  5.6194e-01, -4.9146e-01,  7.2476e-01, -6.3816e-01,\n",
      "          7.8825e-01, -4.8224e-01, -9.8791e-01, -3.1777e-01,  6.9717e-01,\n",
      "          7.2913e-01, -6.2339e-01, -6.3784e-01,  6.0367e-01,  5.6177e-02,\n",
      "          9.6964e-01,  9.1225e-01, -4.5676e-01, -1.8580e-02,  6.4981e-01,\n",
      "         -8.6359e-01, -7.3628e-01,  9.3377e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the BERT model and tokenizer\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") \n",
    "\n",
    "# Process a sentence\n",
    "\n",
    "input_text = \"This is a sample sentence.\"\n",
    "\n",
    "encoded_input = tokenizer(input_text, return_tensors=\"pt\")\n",
    "output = model(**encoded_input)\n",
    "\n",
    "print(output) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82459c46-9c75-48a7-95d3-33b1f94ff890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4fa17e8-816d-4eab-af63-bd259bd4c5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.09747565537691116,\n",
       "  'token': 10533,\n",
       "  'token_str': 'carpenter',\n",
       "  'sequence': 'the man worked as a carpenter.'},\n",
       " {'score': 0.052383314818143845,\n",
       "  'token': 15610,\n",
       "  'token_str': 'waiter',\n",
       "  'sequence': 'the man worked as a waiter.'},\n",
       " {'score': 0.04962708428502083,\n",
       "  'token': 13362,\n",
       "  'token_str': 'barber',\n",
       "  'sequence': 'the man worked as a barber.'},\n",
       " {'score': 0.037886086851358414,\n",
       "  'token': 15893,\n",
       "  'token_str': 'mechanic',\n",
       "  'sequence': 'the man worked as a mechanic.'},\n",
       " {'score': 0.037680912762880325,\n",
       "  'token': 18968,\n",
       "  'token_str': 'salesman',\n",
       "  'sequence': 'the man worked as a salesman.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"The man worked as a [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02b0e9a-f9a6-40a3-a6f5-5cfbbd94745c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
